# RahulGPTPlusPlus
# ğŸ§  RBI Circulars Q&A Chatbot (Fully Local RAG)

This project is a **fully offline Retrieval-Augmented Generation (RAG) chatbot** built to parse and understand **RBI circulars (PDFs)**. It allows users to upload circulars and ask domain-specific questions, returning answers based on actual content â€” with **page-level citation tracking**.

Designed for demonstration within **Risk & Compliance departments** (e.g. IARC), this solution runs **completely offline** without any cloud model calls or token-based usage.

---

## ğŸ”§ Features

- âœ… **Fully Local LLM (No Cloud/Token Usage)** using `ctransformers` + Mistral-7B (GGUF)
- âœ… **HuggingFace MiniLM Embeddings** for compact yet accurate semantic search
- âœ… **FAISS Vector Store** for fast, persistent similarity search
- âœ… **PDF Parsing via PyMuPDF** with page-level metadata
- âœ… **Deduplication via SHA-256 Hashing** to prevent reprocessing files
- âœ… **Token-aware Prompt Construction** capped at 512 tokens (100 reserved for output)
- âœ… **Fallback LLM Answers** for vague queries or missing context
- âœ… **Source & Page Number Tracking** in chatbot responses

---

## ğŸ“‚ Project Structure

RBI_RAG_DEMO/
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ mistral/ # Local Mistral GGUF model (e.g. mistral-7b-instruct-v0.1.Q4_K_M.gguf)
â”‚
â”œâ”€â”€ faiss_index/ # FAISS vector DB files (auto-created)
â”œâ”€â”€ vectorstore/ # (Legacy) Optional for Chroma-based experiments
â”œâ”€â”€ hash_store.json # Tracks processed PDFs to avoid duplicates
â”‚
â”œâ”€â”€ QA_bot.py # Main Gradio app (local RAG chatbot)
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md # You're reading it!

---

## ğŸ–¥ï¸ How It Works

1. **PDF Upload**
   - Users drag and drop RBI circulars in `.pdf` format.
   - Pages are parsed using PyMuPDF with page number metadata.

2. **Chunking & Compression**
   - Text is split using `RecursiveCharacterTextSplitter` and lightly compressed (remove blank lines, simplify legal terms).

3. **Vector Embedding & Storage**
   - Text chunks are embedded using `sentence-transformers/all-MiniLM-L6-v2`.
   - Stored locally in a FAISS vector index with metadata (filename + page).

4. **Question Answering (RAG)**
   - On user query:
     - Top 5 similar chunks are retrieved.
     - Token-aware prompt is created with source tagging.
     - Answer is generated via `ctransformers` running a local Mistral model.

5. **Fallback Handling**
   - If no good matches are found, LLM gives a generic but context-aware response.

---
